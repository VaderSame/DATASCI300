{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - GPT\n",
    "\n",
    "In this assignment, you will use various transformer models for semantic search and for language generation. We will be using the `transformers` python package from huggingface; **note** that this package will automatically download language models as required the first time the code is run, and they can be quite large. (The entire assignment might download a few GB.) You might want to do this on campus, depending on your internet situation.\n",
    "\n",
    "This assignment is to be done individually. You may discuss the project with your classmates, but the work you turn in should be your own.\n",
    "\n",
    "\n",
    "# Using Generative Language Models\n",
    "\n",
    "## Goal\n",
    "\n",
    "To learn about how generative language models can be used in practice, focusing on GPT-2, which is feasible to run locally without a graphics card.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This part uses the `transformers` package which can be installed with conda or pip.\n",
    "\n",
    "## Questions (100 pts)\n",
    "\n",
    "1. Write a script that generates a \"story\" using a local GPT-2 model. Your story should: 1) be at least 100 words long; 2) not have repeated phrases; and 3) be the same every time your script is run. It might be nonsensical and/or hilarious. Use the skeleton code provided below as a starting point, and <https://huggingface.co/blog/how-to-generate> as a reference document.\n",
    "\n",
    "## Part 2 Deliverables\n",
    "\n",
    "Submit your notebook as an attachment on OWL as well as a PDF version of the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "# Checklist\n",
    "\n",
    "Your owl submission should include the following attachments and no additional files:\n",
    "```\n",
    "Assignment9.ipynb\n",
    "Assignment9.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, set_seed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do that. But I'll\n",
      "0: I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do that. But I'll\n",
      "1: I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do that. But I do\n",
      "2: I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do that. But I want\n",
      "3: I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do it. I've been\n",
      "4: I enjoy my local fight club and iced tea, but I don't want to go to the gym.\n",
      "\n",
      "\"I've got a lot of friends who are fighters, so I'm not going to do that. I've been\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy my local fight club and Â I love my local fight club. I love my local fight club. I love my local fight club. I love my local fight club. I love my local fight club. I love my local\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy my local fight club and ', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My GPT-2 Story:\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_decoded_tokens(dt):\n",
    "    display(Markdown(dt))\n",
    "\n",
    "print(\"My GPT-2 Story:\")\n",
    "print(\"---------------\")\n",
    "\n",
    "## Replace 'None' with your story; this just wraps the text\n",
    "## to make it easier to read\n",
    "show_decoded_tokens(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
