{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9 - GPT\n",
    "\n",
    "In this assignment, you will use various transformer models for semantic search and for language generation. We will be using the `transformers` python package from huggingface; **note** that this package will automatically download language models as required the first time the code is run, and they can be quite large. (The entire assignment might download a few GB.) You might want to do this on campus, depending on your internet situation.\n",
    "\n",
    "This assignment is to be done individually. You may discuss the project with your classmates, but the work you turn in should be your own.\n",
    "\n",
    "\n",
    "# Using Generative Language Models\n",
    "\n",
    "## Goal\n",
    "\n",
    "To learn about how generative language models can be used in practice, focusing on GPT-2, which is feasible to run locally without a graphics card.\n",
    "\n",
    "## Setup\n",
    "\n",
    "This part uses the `transformers` package which can be installed with conda or pip.\n",
    "\n",
    "## Questions (100 pts)\n",
    "\n",
    "1. Write a script that generates a \"story\" using a local GPT-2 model. Your story should: 1) be at least 100 words long; 2) not have repeated phrases; and 3) be the same every time your script is run. It might be nonsensical and/or hilarious. Use the skeleton code provided below as a starting point, and <https://huggingface.co/blog/how-to-generate> as a reference document.\n",
    "\n",
    "## Part 2 Deliverables\n",
    "\n",
    "Submit your notebook as an attachment on OWL as well as a PDF version of the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "# Checklist\n",
    "\n",
    "Your owl submission should include the following attachments and no additional files:\n",
    "```\n",
    "Assignment9.ipynb\n",
    "Assignment9.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers\n",
    "%pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, set_seed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_decoded_tokens(dt):\n",
    "    display(Markdown(dt))\n",
    "\n",
    "print(\"My GPT-2 Story:\")\n",
    "print(\"---------------\")\n",
    "\n",
    "## Replace 'None' with your story; this just wraps the text\n",
    "## to make it easier to read\n",
    "show_decoded_tokens(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
