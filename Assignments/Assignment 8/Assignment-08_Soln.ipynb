{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8: Image Classification Experiments\n",
    "\n",
    "In this notebook, we're going to explore the use of a few different ways of setting up an image classification model. The images labels are available: http://cs231n.stanford.edu/tiny-imagenet-200.zip. The training set contains 500 images for each of 200 different classes. The validation set contains 50 images for each of the 200 classes. **You must download and unzip this file, putting the resulting directory in the same directory as your assignment notebook.**\n",
    "\n",
    "Each of the images is a 64x64 pixel image (4096) pixels, each with a rgb value, resulting in 12288 values describing each image.\n",
    "\n",
    "First, we're going to load images from the tiny-imagenet-200 folder into a *flattened* format that is suitable for training any of the scikit-learn classifier models, such as a tree or logistic regression.\n",
    "\n",
    "Later, we'll take advantage of data loading functions included in PyTorch that will preserve the 2D-shape of images and load *batches* instead of the entire training or validation set all at once.\n",
    "\n",
    "**This assignment has one, open-ended question where you are asked to investigate a change to a neural network architecture and see what happens. The code provided will show you how to load in the image data, train neural networks, and evaluate them.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "You will need to install the `pytorch` and `torchvision` packages into your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper and Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def live_plot(loss, train_acc, valid_acc=None, figsize=(7,5), title=''):\n",
    "    clear_output(wait=True)\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    ax1.plot(loss, label='Training Loss', color='red')\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax1.set_ylabel('Cross Entropy Loss')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(train_acc, label='Training Accuracy', color='green')\n",
    "    if valid_acc is not None:\n",
    "        ax2.plot(valid_acc, label='Validation Accuracy', color='blue')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def load_train_dataset():\n",
    "    data_path = './tiny-imagenet-200/train/'\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=torchvision.transforms.ToTensor()\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return train_loader, train_dataset.class_to_idx\n",
    "\n",
    "def load_valid_dataset(class_to_idx):\n",
    "    data_path = './tiny-imagenet-200/val/images'\n",
    "    label_file = open('./tiny-imagenet-200/val/val_annotations.txt', 'r')\n",
    "    label_df = pd.read_csv(label_file, sep='\\s+', header=None)\n",
    "    label_df[1] = label_df[1].apply(lambda x : class_to_idx[x])\n",
    "    valid_data = []\n",
    "    for row in label_df.iterrows():\n",
    "        image = Image.open(f'{data_path}/{row[1][0]}')\n",
    "        image = torchvision.transforms.functional.to_tensor(image)\n",
    "        label = row[1][1]\n",
    "        valid_data.append((image, label))\n",
    "    return valid_data\n",
    "\n",
    "def load_data_np(batches=100):\n",
    "    # Load a subsample of training data and all of the validation data into a flattened NumPy format.\n",
    "    train_data_np = []\n",
    "    train_labels = []\n",
    "    batch_limit = batches\n",
    "    for index, (data, label) in enumerate(train_dataset):\n",
    "        for i in range(data.shape[0]):\n",
    "            train_data_np.append(data[i].detach().numpy().flatten())\n",
    "            train_labels.append(int(label[i].detach().numpy()))\n",
    "        if index >= batch_limit - 1:\n",
    "            break\n",
    "    train_data_np = np.vstack(train_data_np)\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    valid_data_np = []\n",
    "    valid_labels = []\n",
    "    for data, label in valid_dataset:\n",
    "        if data.shape[0] < 3:\n",
    "            data = torch.cat([data[0], data[0], data[0]], dim=0)  # Convert grayscale images into RGB format\n",
    "        valid_data_np.append(data.detach().numpy().flatten())\n",
    "        valid_labels.append(int(label))\n",
    "    valid_data_np = np.vstack(valid_data_np)\n",
    "    valid_labels = np.array(valid_labels)\n",
    "    return train_data_np, train_labels, valid_data_np, valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a training and validation dataset from tiny-imagenet-200\n",
    "# This file must be in the same folder as the /tiny-imagenet-200 for this to work\n",
    "\n",
    "train_dataset, class_to_idx = load_train_dataset()\n",
    "valid_dataset = load_valid_dataset(class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "label_map = pd.read_csv(\"./tiny-imagenet-200/words.txt\", delimiter='\\t', header=None)\n",
    "label_map = dict(zip(label_map[0], label_map[1]))\n",
    "idx_to_class = {v:k for (k,v) in class_to_idx.items()}\n",
    "# The train_dataset object organizes the training data into batches\n",
    "# We grab the first batch and display a few images\n",
    "index, (data, labels) = next(enumerate(train_dataset))\n",
    "# Show ten random images\n",
    "for image_index in random.sample(range(len(data)),10):\n",
    "    image = data[image_index].detach().numpy().T\n",
    "    label = labels.detach().numpy()[image_index]\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{label_map[idx_to_class[label]]}\")\n",
    "    plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into 'flattened' NumPy arrays\n",
    "train_data_np, train_labels, valid_data_np, valid_labels = load_data_np(batches=20)\n",
    "print(f\"Training Data Shape: {train_data_np.shape}\")\n",
    "print(f\"Training Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Validation Data Shape: {valid_data_np.shape}\")\n",
    "print(f\"Validation Labels Shape: {valid_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Neural Network Architecture\n",
    "        self.dense1 = torch.nn.Linear(in_features=num_features, out_features=num_classes)\n",
    "        self.activation = torch.nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 20\n",
    "\n",
    "# Load data into 'flattened' NumPy arrays\n",
    "train_data_np, train_labels, valid_data_np, valid_labels = load_data_np(batches=batch_limit)\n",
    "\n",
    "batch_size = 32\n",
    "num_features = 64*64*3\n",
    "num_classes = 200\n",
    "max_iter = 50\n",
    "model = LinearModel(num_features, num_classes)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "valid_list = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    epoch_loss = 0\n",
    "    for index, (data, label) in enumerate(train_dataset):\n",
    "        data = data.view(batch_size, -1)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(input=y_pred, target=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch loss ({index+1}/{batch_limit}): {loss.item()}\", end='\\r')\n",
    "        if index >= batch_limit -1:\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        loss_list.append(epoch_loss/batch_limit)\n",
    "        y_pred = model.forward(torch.Tensor(train_data_np.reshape(-1, num_features)))\n",
    "        y_pred = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "        accuracy_list.append(accuracy_score(y_true=train_labels, y_pred=y_pred)*100)\n",
    "        y_pred_val = model.forward(torch.Tensor(valid_data_np.reshape(-1, num_features)))\n",
    "        y_pred_val = torch.argmax(y_pred_val, dim=1).detach().numpy()\n",
    "        valid_list.append(accuracy_score(y_true=valid_labels, y_pred=y_pred_val)*100)\n",
    "        # print(f\"Loss at epoch {i}: {loss.item():.4f}\\tAccuracy: {accuracy_list[-1]*100:.2f}%\", end='\\r')\n",
    "        live_plot(np.array(loss_list), np.array(accuracy_list), valid_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model In PyTorch - 10x Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 200\n",
    "\n",
    "# Load data into 'flattened' NumPy arrays\n",
    "train_data_np, train_labels, valid_data_np, valid_labels = load_data_np(batches=batch_limit)\n",
    "\n",
    "batch_size = 32\n",
    "num_features = 64*64*3\n",
    "num_classes = 200\n",
    "max_iter = 100\n",
    "model = LinearModel(num_features, num_classes)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "valid_list = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    epoch_loss = 0\n",
    "    for index, (data, label) in enumerate(train_dataset):\n",
    "        data = data.view(batch_size, -1)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(input=y_pred, target=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch loss ({index+1}/{batch_limit}): {loss.item()}\", end='\\r')\n",
    "        if index >= batch_limit -1:\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        loss_list.append(epoch_loss/batch_limit)\n",
    "        y_pred = model.forward(torch.Tensor(train_data_np.reshape(-1, num_features)))\n",
    "        y_pred = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "        accuracy_list.append(accuracy_score(y_true=train_labels, y_pred=y_pred)*100)\n",
    "        y_pred_val = model.forward(torch.Tensor(valid_data_np.reshape(-1, num_features)))\n",
    "        y_pred_val = torch.argmax(y_pred_val, dim=1).detach().numpy()\n",
    "        valid_list.append(accuracy_score(y_true=valid_labels, y_pred=y_pred_val)*100)\n",
    "        # print(f\"Loss at epoch {i}: {loss.item():.4f}\\tAccuracy: {accuracy_list[-1]*100:.2f}%\", end='\\r')\n",
    "        live_plot(np.array(loss_list), np.array(accuracy_list), valid_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Neural Network With One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_np, train_labels, valid_data_np, valid_labels = load_data_np(batches=200)\n",
    "print(f\"Training Data Shape: {train_data_np.shape}\")\n",
    "print(f\"Training Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Validation Data Shape: {valid_data_np.shape}\")\n",
    "print(f\"Validation Labels Shape: {valid_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Neural Network Architecture\n",
    "        self.dense1 = torch.nn.Linear(in_features=num_features, out_features=400)\n",
    "        self.activation1 = torch.nn.LogSigmoid()\n",
    "        self.dense2 = torch.nn.Linear(in_features=400, out_features=num_classes)\n",
    "        self.activation2 = torch.nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.activation2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 200\n",
    "batch_size = 32\n",
    "num_features = 64*64*3\n",
    "num_classes = 200\n",
    "max_iter = 100\n",
    "model = NonLinearModel(num_features, num_classes)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "valid_list = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    epoch_loss = 0\n",
    "    for index, (data, label) in enumerate(train_dataset):\n",
    "        data = data.view(batch_size, -1)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(input=y_pred, target=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch loss ({index+1}/{batch_limit}): {loss.item()}\", end='\\r')\n",
    "        if index >= batch_limit -1:\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        loss_list.append(epoch_loss/batch_limit)\n",
    "        y_pred = model.forward(torch.Tensor(train_data_np.reshape(-1, num_features)))\n",
    "        y_pred = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "        accuracy_list.append(accuracy_score(y_true=train_labels, y_pred=y_pred)*100)\n",
    "        y_pred_val = model.forward(torch.Tensor(valid_data_np.reshape(-1, num_features)))\n",
    "        y_pred_val = torch.argmax(y_pred_val, dim=1).detach().numpy()\n",
    "        valid_list.append(accuracy_score(y_true=valid_labels, y_pred=y_pred_val)*100)\n",
    "        # print(f\"Loss at epoch {i}: {loss.item():.4f}\\tAccuracy: {accuracy_list[-1]*100:.2f}%\", end='\\r')\n",
    "        live_plot(np.array(loss_list), np.array(accuracy_list), valid_list) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = torch.nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_limit = 200\n",
    "num_features = train_data_np[0].shape  # (64, 64, 3)\n",
    "num_classes = 200\n",
    "max_iter = 100\n",
    "model = ConvNet(64, 64, num_classes)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "valid_list = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    epoch_loss = 0\n",
    "    for index, (data, label) in enumerate(train_dataset):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(input=y_pred, target=label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"Batch loss ({index+1}/{batch_limit}): {loss.item()}\", end='\\r')\n",
    "        if index >= batch_limit -1:\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        loss_list.append(epoch_loss/batch_limit)\n",
    "        y_pred = model.forward(torch.Tensor(train_data_np.reshape(-1, 3, 64, 64)))\n",
    "        y_pred = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "        accuracy_list.append(accuracy_score(y_true=train_labels, y_pred=y_pred)*100)\n",
    "        y_pred_val = model.forward(torch.Tensor(valid_data_np.reshape(-1, 3, 64, 64)))\n",
    "        y_pred_val = torch.argmax(y_pred_val, dim=1).detach().numpy()\n",
    "        valid_list.append(accuracy_score(y_true=valid_labels, y_pred=y_pred_val)*100)\n",
    "        # print(f\"Loss at epoch {i}: {loss.item():.4f}\\tAccuracy: {accuracy_list[-1]*100:.2f}%\", end='\\r')\n",
    "        live_plot(np.array(loss_list), np.array(accuracy_list), valid_list) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Think about a modification to one of the above models you would like to make. You could decide to change the data preparation or splitting, the model architecture, loss function, learning algorithm, or any associated paramter. Implement an experiment that enables you to evaluate the impact of your modification. Make sure that your experiment is only studying the effects of one change. For example, you could compare the learning progress of a neural network model with one hidden layer vs. two hidden layers. In this case, you should make sure to use exactly the same training and validation datasets, the same loss function, and the same learning algorithm.\n",
    "\n",
    "For each experimental condition, you can use the provided code to generate a learning progress plot, but feel free to explore other tools for evaluation, such as a [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py). This would be especially helpful to understand in what way your model is making classification errors.\n",
    "\n",
    "**For submission**, *describe both the model you are starting (control) with* and the *modification you are evaluating (experimental).* What effect do you think your modification will have on training? On overall model performance? Next give a short description of what data you will use in your experiments (e.g. how many images, their format, and how many classes). Then, give a summary of results, clearly indicating the experimental condition to which they belong (i.e. control or experimental).\n",
    "\n",
    "Comment on your results. Did your modification improve your image classifier? What evidence do you have to support this? Based on your observations or any other sources, what additional changes do you think could further improve your model? Do you think the training data you used contained sufficient training examples for your model to generalize well? Justify your claim.\n",
    "\n",
    "Format your sumbission as a single Jupyter Notebook *and* corresponding PDF file. Use OWL to submit your assignment. Keep your submission brief: a single page (or equivalent) submission should be adequate.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Depending on your computer's hardware, you may need to scale back your experiments. You can try reducing the number of training iterations, the size of your training set, or the size of your model architecture. Just focus on creating a controlled experiment and explain that you had to use less data. \n",
    "* Along these lines, don't run experiments that will take too long to run. Test your experiments on a small number of training iterations (5 or so) to get a sense of how long each iteration takes. Even with relatively small training sets, it could take a few hours for model training to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What model are you starting with?\n",
    "\n",
    "#### Explain in a short paragraph which of the models (and training strategies) above you will use for your comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What modification are you making? Why?\n",
    "\n",
    "#### Explain in a short paragraph what modification(s) you are making to the learning model, and briefly explain why you are making those modifications. (For example: Did you hear somewhere that it would help? Do you have a theoretical reason for the modification? Do you think learning might be faster? Do you think performance might be higher?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data will you use?\n",
    "\n",
    "#### Will use use the whole dataset or just a subset? Are you restricting the possible classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results and your conclusions\n",
    "\n",
    "#### Summarize what happened when you ran your experiment. Explain whether you found the result surprising or not. Explain the implications of your findings. (For example, is your modification better? Worse? Better but slower to train? Worse but faster?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
